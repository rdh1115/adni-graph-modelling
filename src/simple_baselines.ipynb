{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T00:55:27.930776Z",
     "start_time": "2024-10-08T00:55:05.059449Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from iopath.common.file_io import g_pathmgr as pathmgr\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, PredefinedSplit\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "import baselines\n",
    "import model_finetune\n",
    "from gmae_st.data.get_dataset import get_dataset\n",
    "from gmae_st.utils import misc\n",
    "from gmae_st.data.utils import DX_DICT\n",
    "from data.utils import collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ed1d02d2ca84225",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T00:55:27.948056Z",
     "start_time": "2024-10-08T00:55:27.933305Z"
    }
   },
   "outputs": [],
   "source": [
    "class DictArgs:\n",
    "    def __init__(self, d):\n",
    "        for key, value in d.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "\n",
    "def get_vis_dataset(\n",
    "        dataset_dir,\n",
    "        graph_token,\n",
    "        n_hist,\n",
    "        n_pred,\n",
    "        num_visits,\n",
    "        filter_list,\n",
    "        filter_diagnosis,\n",
    "        include_pet_volume,\n",
    "        norm\n",
    "):\n",
    "    dataset_dict = get_dataset(\n",
    "        dataset_type='brain',\n",
    "        dataset_name='ADNI',\n",
    "        data_dir=dataset_dir,\n",
    "        n_hist=n_hist,\n",
    "        n_pred=n_pred,\n",
    "        num_visits=num_visits,\n",
    "        task='pred',\n",
    "        filter_list=filter_list,\n",
    "        filter_diagnosis=filter_diagnosis,\n",
    "        graph_token=graph_token,\n",
    "        mode='finetune',\n",
    "        include_pet_volume=include_pet_volume,\n",
    "        norm=norm\n",
    "    )\n",
    "    dataset_train = dataset_dict['train_dataset']\n",
    "    dataset_val = dataset_dict['valid_dataset']\n",
    "    dataset_test = dataset_dict['test_dataset']\n",
    "    scaler = dataset_train.scaler\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "    sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "    sampler_test = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "    data_sample = dataset_train[0]\n",
    "    node_feature_dim = data_sample['x'].shape[-1]\n",
    "    num_nodes = data_sample['adj'].shape[0]\n",
    "    num_edges = len(data_sample['edge_attr'])\n",
    "\n",
    "    # account for data.utils.collator changes\n",
    "    num_spatial = torch.max(data_sample['spatial_pos']).item() + 1\n",
    "    num_in_degree = torch.max(data_sample['in_degree']).item() + 1\n",
    "    num_out_degree = torch.max(data_sample['out_degree']).item() + 1\n",
    "    graph_info = {\n",
    "        'node_feature_dim': node_feature_dim,\n",
    "        'num_nodes': num_nodes,\n",
    "        'num_edges': num_edges,\n",
    "        'num_spatial': num_spatial,\n",
    "        'num_in_degree': num_in_degree,\n",
    "        'num_out_degree': num_out_degree\n",
    "    }\n",
    "    data_loader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        sampler=sampler_train,\n",
    "        batch_size=4,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=partial(\n",
    "            collator,\n",
    "            max_node=num_nodes,\n",
    "            spatial_pos_max=num_spatial,\n",
    "            graph_token=graph_token,\n",
    "            scaler=scaler,\n",
    "        ),\n",
    "    )\n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        sampler=sampler_val,\n",
    "        batch_size=4,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=partial(\n",
    "            collator,\n",
    "            max_node=num_nodes,\n",
    "            spatial_pos_max=num_spatial,\n",
    "            graph_token=graph_token,\n",
    "            scaler=scaler,\n",
    "        ),\n",
    "    )\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        sampler=sampler_test,\n",
    "        batch_size=4,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=partial(\n",
    "            collator,\n",
    "            max_node=num_nodes,\n",
    "            spatial_pos_max=num_spatial,\n",
    "            graph_token=graph_token,\n",
    "            scaler=scaler,\n",
    "        ),\n",
    "    )\n",
    "    return data_loader_train, data_loader_val, data_loader_test, graph_info\n",
    "\n",
    "\n",
    "def prepare_data(data_loader, args):\n",
    "    \"\"\"\n",
    "    Prepares the input and target data for sklearn.\n",
    "    Flattens the input tensor and converts it into numpy arrays for sklearn compatibility.\n",
    "    \"\"\"\n",
    "    all_samples = []\n",
    "    all_targets = []\n",
    "    device = torch.device('cpu')\n",
    "    for batch in data_loader:\n",
    "        batch = misc.prepare_batch(batch, device=device)\n",
    "        samples, targets, target_shape = misc.get_samples_targets(\n",
    "            batch=batch,\n",
    "            task='pred',\n",
    "            device=device,\n",
    "            args=args\n",
    "        )\n",
    "\n",
    "        assert samples.shape[0] == targets.shape[\n",
    "            0], f'batch size of samples {samples.shape[0]} does not match targets {targets.shape[0]}'\n",
    "        assert samples.shape[-1] == targets.shape[\n",
    "            -1], f'feature dimension of samples {samples.shape[-1]} does not match targets {targets.shape[-1]}'\n",
    "\n",
    "        _, T, _, _ = samples.shape\n",
    "        N, P, V, D = target_shape\n",
    "        samples = samples.view(N * T, V * D).numpy()  # Flatten samples for sklearn\n",
    "        targets = targets.view(N, P * V * D).numpy()  # Flatten targets\n",
    "\n",
    "        all_samples.append(samples)\n",
    "        all_targets.append(targets)\n",
    "\n",
    "    # Combine all batches into single arrays\n",
    "    all_samples = np.vstack(all_samples)\n",
    "    all_targets = np.vstack(all_targets)\n",
    "\n",
    "    return all_samples, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a204d1d5ca13378",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T00:55:27.963109Z",
     "start_time": "2024-10-08T00:55:27.949806Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define default parameter grids for each model\n",
    "PARAM_GRIDS = {\n",
    "    'Linear Regression': {},  # No hyperparameters to tune for basic LinearRegression\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'Support Vector Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'K-Nearest Neighbors': {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.5],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.5],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def evaluate_metrics(target, output):\n",
    "    metrics = {}\n",
    "\n",
    "    # Mask out node values that are zero to avoid numerical errors in MAPE\n",
    "    mask = (target != 0)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = np.abs(output - target)\n",
    "    mae = np.where(mask, mae, 0)  # Apply mask\n",
    "    metrics['MAE'] = np.mean(mae)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.square(output - target)\n",
    "    rmse = np.where(mask, rmse, 0)  # Apply mask\n",
    "    metrics['RMSE'] = np.sqrt(np.mean(rmse))\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = np.abs((output - target) / target)\n",
    "    mape = np.where(mask, mape, 0)  # Apply mask\n",
    "    metrics['MAPE'] = np.mean(mape)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def grid_search(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_val,\n",
    "        y_val,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        model_hyperparameters=None,\n",
    "        seed=0,\n",
    "):\n",
    "    # Combine train and validation data\n",
    "    X_trainval = np.concatenate([X_train, X_val], axis=0)\n",
    "    y_trainval = np.concatenate([y_train, y_val], axis=0)\n",
    "\n",
    "    # Create a test_fold array: -1 for training samples, 0 for validation samples\n",
    "    test_fold = np.hstack([np.full(X_train.shape[0], -1), np.full(X_val.shape[0], 0)])\n",
    "\n",
    "    # PredefinedSplit object\n",
    "    ps = PredefinedSplit(test_fold)\n",
    "\n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Random Forest': RandomForestRegressor(),\n",
    "        'Support Vector Regression': SVR(),\n",
    "        'K-Nearest Neighbors': KNeighborsRegressor(),\n",
    "        'Decision Tree': DecisionTreeRegressor(),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(),\n",
    "        'XGBoost': xgb.XGBRegressor(),\n",
    "    }\n",
    "\n",
    "    # GridSearchCV results storage\n",
    "    grid_results = []\n",
    "    # Load model hyperparameters\n",
    "    param_grids = PARAM_GRIDS\n",
    "    if model_hyperparameters:\n",
    "        param_grids = model_hyperparameters\n",
    "\n",
    "    # Train and evaluate models using GridSearchCV and KFold\n",
    "    for model_name, model in models.items():\n",
    "        if model_name in param_grids:\n",
    "            param_grid = param_grids[model_name]\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=model,\n",
    "                param_grid=param_grid,\n",
    "                cv=ps,\n",
    "            )\n",
    "\n",
    "            # Fit model using GridSearchCV\n",
    "            grid_search.fit(X_trainval, y_trainval)\n",
    "\n",
    "            # Get best model from grid search\n",
    "            best_model = grid_search.best_estimator_\n",
    "\n",
    "            # Make predictions on the test set\n",
    "            y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "            # Calculate MSE, RMSE\n",
    "            metrics = evaluate_metrics(y_test, y_test_pred)\n",
    "            mae, rmse, mape = metrics['MAE'], metrics['RMSE'], metrics['MAPE']\n",
    "            # Store results\n",
    "            grid_results.append({\n",
    "                'Model': model_name,\n",
    "                'Best Params': grid_search.best_params_,\n",
    "                'Test MAE': mae,\n",
    "                'Test RMSE': rmse,\n",
    "                'Test MAPE': mape\n",
    "            })\n",
    "\n",
    "            # Output best parameters and results for each model\n",
    "            print(f'Best parameters for {model_name}: {grid_search.best_params_}')\n",
    "            print(f'{model_name} - Test MAE: {mae:.4f}, Test RMSE: {rmse:.4f}, Test MAPE: {mape:.4f}')\n",
    "        else:\n",
    "            print(f'No parameters found for {model_name}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e60a16c1999b0a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T00:55:28.087690Z",
     "start_time": "2024-10-08T00:55:27.965625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting ADNI data with \n",
      "num_visits: 3, \n",
      "Amyloid-Beta PET scans\n",
      "total subjects: 330\n",
      "train subjects:  231 val subjects:  33 test subjects:  66\n",
      "Using normalization with mean: [1.12399331], std: [0.23197721]\n",
      " > ADNI loaded!\n",
      "{'train_dataset': GraphTemporalDataset(398), 'valid_dataset': GraphTemporalDataset(55), 'test_dataset': GraphTemporalDataset(120), 'class_init_prob': None}\n",
      " > dataset info ends\n",
      "{'dataset_dir': '', 'n_hist': 1, 'n_pred': 2, 'num_visits': 3, 'filter_list': (0, 1, 0), 'filter_diagnosis': False, 'include_pet_volume': False, 'norm': True, 'graph_token': False, 'node_feature_dim': 1, 'num_nodes': 68, 'num_edges': 1394, 'num_spatial': 19, 'num_in_degree': 42, 'num_out_degree': 42, 'dataset_type': 'brain'}\n"
     ]
    }
   ],
   "source": [
    "n_hist, n_pred, num_visits = 1, 2, 3\n",
    "filter_list = (0, 1, 0)\n",
    "filter_diagnosis, include_pet_volume = False, False\n",
    "norm = True\n",
    "graph_token = False\n",
    "dataset_dir = ''\n",
    "dataset_args = {\n",
    "    'dataset_dir': dataset_dir,\n",
    "    'n_hist': n_hist,\n",
    "    'n_pred': n_pred,\n",
    "    'num_visits': num_visits,\n",
    "    'filter_list': filter_list,\n",
    "    'filter_diagnosis': filter_diagnosis,\n",
    "    'include_pet_volume': include_pet_volume,\n",
    "    'norm': norm,\n",
    "    'graph_token': graph_token,\n",
    "}\n",
    "loader_train, loader_val, loader_test, graph_info = get_vis_dataset(\n",
    "    **dataset_args\n",
    ")\n",
    "dataset_args.update(graph_info)\n",
    "dataset_args['dataset_type'] = 'brain'\n",
    "print(dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b2f9873becbc60",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-08T00:55:28.089648Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "x_train, y_train = prepare_data(loader_train, DictArgs(dataset_args))\n",
    "x_val, y_val = prepare_data(loader_val, DictArgs(dataset_args))\n",
    "x_test, y_test = prepare_data(loader_test, DictArgs(dataset_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47ede4102534868",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)\n",
    "x_train[0][0], y_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ca88a72982c08",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "grid_search(\n",
    "    x_train, y_train,\n",
    "    x_val, y_val,\n",
    "    x_test, y_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad7015d2b61875c",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
